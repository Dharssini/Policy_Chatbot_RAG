{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dharssinikarthikeyan/Projects/Improva_Task'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dharssinikarthikeyan/Projects/Improva_Task'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Data From the PDF File\n",
    "def load_pdf_file(data):\n",
    "    loader= DirectoryLoader(data,\n",
    "                            glob=\"*.pdf\",\n",
    "                            loader_cls=PyPDFLoader)\n",
    "\n",
    "    documents=loader.load()\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 147 0 (offset 0)\n",
      "Ignoring wrong pointing object 153 0 (offset 0)\n",
      "Ignoring wrong pointing object 161 0 (offset 0)\n",
      "Ignoring wrong pointing object 163 0 (offset 0)\n",
      "Ignoring wrong pointing object 279 0 (offset 0)\n",
      "Ignoring wrong pointing object 283 0 (offset 0)\n",
      "Ignoring wrong pointing object 287 0 (offset 0)\n",
      "Ignoring wrong pointing object 324 0 (offset 0)\n",
      "Ignoring wrong pointing object 694 0 (offset 0)\n",
      "Ignoring wrong pointing object 698 0 (offset 0)\n",
      "Ignoring wrong pointing object 706 0 (offset 0)\n",
      "Ignoring wrong pointing object 708 0 (offset 0)\n",
      "Ignoring wrong pointing object 751 0 (offset 0)\n",
      "Ignoring wrong pointing object 754 0 (offset 0)\n",
      "Ignoring wrong pointing object 756 0 (offset 0)\n",
      "Ignoring wrong pointing object 760 0 (offset 0)\n",
      "Ignoring wrong pointing object 826 0 (offset 0)\n",
      "Ignoring wrong pointing object 912 0 (offset 0)\n",
      "Ignoring wrong pointing object 937 0 (offset 0)\n",
      "Ignoring wrong pointing object 941 0 (offset 0)\n",
      "Ignoring wrong pointing object 949 0 (offset 0)\n",
      "Ignoring wrong pointing object 987 0 (offset 0)\n",
      "Ignoring wrong pointing object 992 0 (offset 0)\n",
      "Ignoring wrong pointing object 994 0 (offset 0)\n",
      "Ignoring wrong pointing object 1021 0 (offset 0)\n",
      "Ignoring wrong pointing object 1246 0 (offset 0)\n",
      "Ignoring wrong pointing object 1248 0 (offset 0)\n",
      "Ignoring wrong pointing object 1250 0 (offset 0)\n",
      "Ignoring wrong pointing object 1310 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "extracted_data=load_pdf_file(data='Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Data into Text Chunks\n",
    "def text_split(extracted_data):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    text_chunks=text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Chunks 550\n"
     ]
    }
   ],
   "source": [
    "text_chunks=text_split(extracted_data)\n",
    "print(\"Length of Text Chunks\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the Embeddings from Hugging Face\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qk/nfy6fxm92l34ht5sh2br5qdw0000gn/T/ipykernel_6228/1196424635.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n"
     ]
    }
   ],
   "source": [
    "embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length 384\n"
     ]
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")\n",
    "print(\"Length\", len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03447727486491203,\n",
       " 0.031023185700178146,\n",
       " 0.006734943483024836,\n",
       " 0.026108991354703903,\n",
       " -0.039362020790576935,\n",
       " -0.16030248999595642,\n",
       " 0.06692396104335785,\n",
       " -0.0064414748921990395,\n",
       " -0.04745050519704819,\n",
       " 0.01475885696709156,\n",
       " 0.07087533175945282,\n",
       " 0.05552756041288376,\n",
       " 0.019193293526768684,\n",
       " -0.026251325383782387,\n",
       " -0.010109511204063892,\n",
       " -0.026940522715449333,\n",
       " 0.02230747975409031,\n",
       " -0.02222663350403309,\n",
       " -0.14969263970851898,\n",
       " -0.017493100836873055,\n",
       " 0.007676226552575827,\n",
       " 0.05435233935713768,\n",
       " 0.003254474140703678,\n",
       " 0.03172595798969269,\n",
       " -0.08462139964103699,\n",
       " -0.029405970126390457,\n",
       " 0.05159565806388855,\n",
       " 0.04812402278184891,\n",
       " -0.0033147968351840973,\n",
       " -0.05827918276190758,\n",
       " 0.041969284415245056,\n",
       " 0.022210702300071716,\n",
       " 0.12818880379199982,\n",
       " -0.022338924929499626,\n",
       " -0.011656263843178749,\n",
       " 0.06292839348316193,\n",
       " -0.032876282930374146,\n",
       " -0.09122605621814728,\n",
       " -0.03117532655596733,\n",
       " 0.05269956961274147,\n",
       " 0.04703480750322342,\n",
       " -0.08420305699110031,\n",
       " -0.030056172981858253,\n",
       " -0.02074481174349785,\n",
       " 0.009517787024378777,\n",
       " -0.003721744753420353,\n",
       " 0.007343323435634375,\n",
       " 0.039324335753917694,\n",
       " 0.0932740643620491,\n",
       " -0.003788587637245655,\n",
       " -0.05274208262562752,\n",
       " -0.058058205991983414,\n",
       " -0.006864395923912525,\n",
       " 0.005283238831907511,\n",
       " 0.0828929990530014,\n",
       " 0.019362738355994225,\n",
       " 0.0062844655476510525,\n",
       " -0.010330751538276672,\n",
       " 0.009032376110553741,\n",
       " -0.03768378123641014,\n",
       " -0.04520609974861145,\n",
       " 0.02401636354625225,\n",
       " -0.006944145541638136,\n",
       " 0.01349162682890892,\n",
       " 0.10005497932434082,\n",
       " -0.0716838389635086,\n",
       " -0.021695038303732872,\n",
       " 0.03161851316690445,\n",
       " -0.05163462832570076,\n",
       " -0.08224769681692123,\n",
       " -0.06569333374500275,\n",
       " -0.009895357303321362,\n",
       " 0.005816420074552298,\n",
       " 0.07355459034442902,\n",
       " -0.03405028581619263,\n",
       " 0.024886086583137512,\n",
       " 0.014488081447780132,\n",
       " 0.02645736187696457,\n",
       " 0.009656742215156555,\n",
       " 0.03021734021604061,\n",
       " 0.052803993225097656,\n",
       " -0.07535990327596664,\n",
       " 0.009897188283503056,\n",
       " 0.029836855828762054,\n",
       " 0.01755557954311371,\n",
       " 0.023091962561011314,\n",
       " 0.0019338703714311123,\n",
       " 0.0014001704985275865,\n",
       " -0.047175951302051544,\n",
       " -0.011194347403943539,\n",
       " -0.11420135200023651,\n",
       " -0.019811976701021194,\n",
       " 0.04026623070240021,\n",
       " 0.002193037886172533,\n",
       " -0.07979223877191544,\n",
       " -0.025382354855537415,\n",
       " 0.09448292851448059,\n",
       " -0.028981134295463562,\n",
       " -0.14500248432159424,\n",
       " 0.23097743093967438,\n",
       " 0.02773112617433071,\n",
       " 0.032111454755067825,\n",
       " 0.03106500953435898,\n",
       " 0.042832862585783005,\n",
       " 0.0642378032207489,\n",
       " 0.032163165509700775,\n",
       " -0.0048767197877168655,\n",
       " 0.05569947510957718,\n",
       " -0.037532392889261246,\n",
       " -0.021505579352378845,\n",
       " -0.028342703357338905,\n",
       " -0.028846895322203636,\n",
       " 0.038353100419044495,\n",
       " -0.01746862381696701,\n",
       " 0.05248527601361275,\n",
       " -0.07487606257200241,\n",
       " -0.03125975281000137,\n",
       " 0.021841542795300484,\n",
       " -0.039895664900541306,\n",
       " -0.008587117306888103,\n",
       " 0.026956627145409584,\n",
       " -0.04849552363157272,\n",
       " 0.011469900608062744,\n",
       " 0.029618270695209503,\n",
       " -0.020572176203131676,\n",
       " 0.013103868812322617,\n",
       " 0.028833432123064995,\n",
       " -3.194200554190157e-33,\n",
       " 0.06478201597929001,\n",
       " -0.018130235373973846,\n",
       " 0.05178994685411453,\n",
       " 0.12198272347450256,\n",
       " 0.02878018282353878,\n",
       " 0.008722026832401752,\n",
       " -0.07052114605903625,\n",
       " -0.016907259821891785,\n",
       " 0.0407397486269474,\n",
       " 0.0421161912381649,\n",
       " 0.025447234511375427,\n",
       " 0.03574623912572861,\n",
       " -0.04914475977420807,\n",
       " 0.0021290781442075968,\n",
       " -0.015546579845249653,\n",
       " 0.050730545073747635,\n",
       " -0.048185307532548904,\n",
       " 0.03588062897324562,\n",
       " -0.004067040979862213,\n",
       " 0.10172472149133682,\n",
       " -0.05597004294395447,\n",
       " -0.010681048966944218,\n",
       " 0.011235816404223442,\n",
       " 0.09068647772073746,\n",
       " 0.0042344690300524235,\n",
       " 0.03513867035508156,\n",
       " -0.009702816605567932,\n",
       " -0.09386518597602844,\n",
       " 0.09285556524991989,\n",
       " 0.008004957810044289,\n",
       " -0.007705434691160917,\n",
       " -0.05208675190806389,\n",
       " -0.012587963603436947,\n",
       " 0.0032669089268893003,\n",
       " 0.006013541016727686,\n",
       " 0.007581611629575491,\n",
       " 0.010517185553908348,\n",
       " -0.08634551614522934,\n",
       " -0.06987876445055008,\n",
       " -0.00253385491669178,\n",
       " -0.0909765437245369,\n",
       " 0.04688732326030731,\n",
       " 0.05207647755742073,\n",
       " 0.007193896919488907,\n",
       " 0.010903622023761272,\n",
       " -0.005229527130723,\n",
       " 0.013937309384346008,\n",
       " 0.021968355402350426,\n",
       " 0.03420855477452278,\n",
       " 0.060224659740924835,\n",
       " 0.0001166820147773251,\n",
       " 0.01473192684352398,\n",
       " -0.07008921355009079,\n",
       " 0.028499027714133263,\n",
       " -0.02760162018239498,\n",
       " 0.010768432170152664,\n",
       " 0.034830980002880096,\n",
       " -0.022487875074148178,\n",
       " 0.009769045747816563,\n",
       " 0.07722780853509903,\n",
       " 0.021588344126939774,\n",
       " 0.11495622247457504,\n",
       " -0.0680011659860611,\n",
       " 0.023761015385389328,\n",
       " -0.015983952209353447,\n",
       " -0.017826979979872704,\n",
       " 0.06439492851495743,\n",
       " 0.03202580288052559,\n",
       " 0.05027025565505028,\n",
       " -0.005913743283599615,\n",
       " -0.03370802477002144,\n",
       " 0.017840370535850525,\n",
       " 0.01657336950302124,\n",
       " 0.06329652667045593,\n",
       " 0.03467720001935959,\n",
       " 0.0464734211564064,\n",
       " 0.09790615737438202,\n",
       " -0.006635462399572134,\n",
       " 0.02520698308944702,\n",
       " -0.07798831164836884,\n",
       " 0.016926409676671028,\n",
       " -0.0009458207059651613,\n",
       " 0.022471873089671135,\n",
       " -0.03825325146317482,\n",
       " 0.09570478647947311,\n",
       " -0.005350717343389988,\n",
       " 0.010469041764736176,\n",
       " -0.11524056643247604,\n",
       " -0.013262496329843998,\n",
       " -0.010709422640502453,\n",
       " -0.08311723172664642,\n",
       " 0.07327358424663544,\n",
       " 0.0493922159075737,\n",
       " -0.008994401432573795,\n",
       " -0.09584559500217438,\n",
       " 3.366150398460503e-33,\n",
       " 0.12493178993463516,\n",
       " 0.019349679350852966,\n",
       " -0.05822572112083435,\n",
       " -0.035988230258226395,\n",
       " -0.05074681341648102,\n",
       " -0.04566239193081856,\n",
       " -0.08260344713926315,\n",
       " 0.14819476008415222,\n",
       " -0.08842113614082336,\n",
       " 0.06027448922395706,\n",
       " 0.05103021115064621,\n",
       " 0.010303132236003876,\n",
       " 0.14121422171592712,\n",
       " 0.03081383928656578,\n",
       " 0.06103311479091644,\n",
       " -0.05285128578543663,\n",
       " 0.13664889335632324,\n",
       " 0.009189903736114502,\n",
       " -0.017325250431895256,\n",
       " -0.012848635204136372,\n",
       " -0.007995281368494034,\n",
       " -0.05098007991909981,\n",
       " -0.052350640296936035,\n",
       " 0.007593068759888411,\n",
       " -0.015166296623647213,\n",
       " 0.01696033962070942,\n",
       " 0.021270548924803734,\n",
       " 0.02055799961090088,\n",
       " -0.12002807855606079,\n",
       " 0.01446180697530508,\n",
       " 0.02675987035036087,\n",
       " 0.02533058635890484,\n",
       " -0.04275466501712799,\n",
       " 0.006768404971808195,\n",
       " -0.01445858832448721,\n",
       " 0.045262016355991364,\n",
       " -0.09147656708955765,\n",
       " -0.019439158961176872,\n",
       " -0.017833461984992027,\n",
       " -0.0549100898206234,\n",
       " -0.05264107137918472,\n",
       " -0.01045901607722044,\n",
       " -0.05201607197523117,\n",
       " 0.020892063155770302,\n",
       " -0.07997036725282669,\n",
       " -0.012111352756619453,\n",
       " -0.05773141235113144,\n",
       " 0.023178264498710632,\n",
       " -0.008031617850065231,\n",
       " -0.025989271700382233,\n",
       " -0.07995667308568954,\n",
       " -0.020728806033730507,\n",
       " 0.048817768692970276,\n",
       " -0.020389171317219734,\n",
       " -0.04917658492922783,\n",
       " 0.014159624464809895,\n",
       " -0.06362210214138031,\n",
       " -0.007807397283613682,\n",
       " 0.016431523486971855,\n",
       " -0.025682521983981133,\n",
       " 0.01338107232004404,\n",
       " 0.026248754933476448,\n",
       " 0.009978369809687138,\n",
       " 0.06322892010211945,\n",
       " 0.0026720957830548286,\n",
       " -0.00658278726041317,\n",
       " 0.016631972044706345,\n",
       " 0.03236646577715874,\n",
       " 0.03794253244996071,\n",
       " -0.03637604042887688,\n",
       " -0.006910967640578747,\n",
       " 0.0001596823858562857,\n",
       " -0.0016335871769115329,\n",
       " -0.027278205379843712,\n",
       " -0.02803804725408554,\n",
       " 0.04968151077628136,\n",
       " -0.028867177665233612,\n",
       " -0.0024180368054658175,\n",
       " 0.014774898067116737,\n",
       " 0.009764556773006916,\n",
       " 0.005797572899609804,\n",
       " 0.013486134819686413,\n",
       " 0.00556792551651597,\n",
       " 0.037227049469947815,\n",
       " 0.007232537027448416,\n",
       " 0.04015621542930603,\n",
       " 0.0815032571554184,\n",
       " 0.07199163734912872,\n",
       " -0.01305615995079279,\n",
       " -0.04288206622004509,\n",
       " -0.011011246591806412,\n",
       " 0.004897758364677429,\n",
       " -0.00922972522675991,\n",
       " 0.035191480070352554,\n",
       " -0.05103502795100212,\n",
       " -1.571437557856825e-08,\n",
       " -0.08862439543008804,\n",
       " 0.0239093154668808,\n",
       " -0.016238760203123093,\n",
       " 0.03170046582818031,\n",
       " 0.027284203097224236,\n",
       " 0.05246879905462265,\n",
       " -0.04707096144556999,\n",
       " -0.058847419917583466,\n",
       " -0.06320823729038239,\n",
       " 0.040888525545597076,\n",
       " 0.04982791841030121,\n",
       " 0.10655166953802109,\n",
       " -0.07450228184461594,\n",
       " -0.012495423667132854,\n",
       " 0.01837068423628807,\n",
       " 0.039474111050367355,\n",
       " -0.024797910824418068,\n",
       " 0.014516254886984825,\n",
       " -0.03706919029355049,\n",
       " 0.020015720278024673,\n",
       " -4.8585523472866043e-05,\n",
       " 0.009866517968475819,\n",
       " 0.024838803336024284,\n",
       " -0.05245813727378845,\n",
       " 0.029314149171113968,\n",
       " -0.08719192445278168,\n",
       " -0.014499801211059093,\n",
       " 0.026019031181931496,\n",
       " -0.018746381625533104,\n",
       " -0.07620511949062347,\n",
       " 0.03504328802227974,\n",
       " 0.10363954305648804,\n",
       " -0.028050540015101433,\n",
       " 0.012718193233013153,\n",
       " -0.07632554322481155,\n",
       " -0.01865239068865776,\n",
       " 0.024976715445518494,\n",
       " 0.08144538849592209,\n",
       " 0.06875890493392944,\n",
       " -0.06405661255121231,\n",
       " -0.08389391005039215,\n",
       " 0.061362382024526596,\n",
       " -0.033545564860105515,\n",
       " -0.10615339130163193,\n",
       " -0.040080584585666656,\n",
       " 0.03253018483519554,\n",
       " 0.07662486284971237,\n",
       " -0.07301616668701172,\n",
       " 0.0003376000968273729,\n",
       " -0.040871553122997284,\n",
       " -0.07578852027654648,\n",
       " 0.027527660131454468,\n",
       " 0.07462547719478607,\n",
       " 0.017717288807034492,\n",
       " 0.09121840447187424,\n",
       " 0.11022017896175385,\n",
       " 0.0005697730812244117,\n",
       " 0.05146332457661629,\n",
       " -0.014551307074725628,\n",
       " 0.03323204442858696,\n",
       " 0.02379223145544529,\n",
       " -0.022889813408255577,\n",
       " 0.0389375314116478,\n",
       " 0.030206799507141113]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ.get('PINECONE_API_KEY')\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = 'pcsk_5FfPWM_QyGBshS5pjqcsm9brNBMJFP1rL76HLW8hPbcsAiWEbAeFk7yuTkE8bP48mjo99R'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "# import Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"policybot\"\n",
    "\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, \n",
    "    metric=\"cosine\", \n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\", \n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
    "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks,\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Existing index \n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "# Embed each chunk and upsert the embeddings into your Pinecone index.\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x10f929330>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.retrievers import ContextualCompressionRetriever\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# from langchain.document_compressors import HuggingFaceRerank\n",
    "\n",
    "# # Initialize the embedding model\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "\n",
    "# # Initialize the vector store\n",
    "# docsearch = Pinecone.from_existing_index(index_name=\"your_index\", embedding=embeddings)\n",
    "\n",
    "# # Initialize the base retriever\n",
    "# retriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "\n",
    "# # Use HuggingFaceRerank as the compressor\n",
    "# reranker = HuggingFaceRerank(model_name=\"BAAI/bge-reranker-base\", top_n=5)\n",
    "\n",
    "# # Apply the reranker in ContextualCompressionRetriever\n",
    "# compressed_retriever = ContextualCompressionRetriever(\n",
    "#     base_retriever=retriever,\n",
    "#     base_compressor=reranker\n",
    "# )\n",
    "\n",
    "# # Query with reranker\n",
    "# retrieved_docs = compressed_retriever.get_relevant_documents(\"Can I use drugs?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Can I use drugs?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the LLM to use LM Studio server\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",  # LM Studio's default server URL\n",
    "    api_key=\"lm-studio-api-key\",  # Placeholder API key; LM Studio may not require authentication\n",
    "    model_name=\"deepseek-r1-distill-qwen-7b\",  # Model name running on LM Studio\n",
    "    temperature=0.4,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# Define the system prompt for the assistant\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question, and include the page number of the source when available. \"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Keep the answer concise, using a maximum of three sentences.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context} (Page: {page_number})\"\n",
    ")\n",
    "\n",
    "# Prepare the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up the question-answering chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Assume 'retriever' is defined and set up properly\n",
    "# This could be from a vector database like Pinecone or FAISS\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the RAG chain with a sample question\n",
    "response = rag_chain.invoke({\"input\": \"How should employees report a potential compliance issue?\"})\n",
    "\n",
    "# Print the response\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some prompts you can use to test your employee policy chatbot based on the policy document:\n",
    "\n",
    "# General Company Information\n",
    "# \t1.\t“What is the mission statement of TriageLogic?”\n",
    "# \t2.\t“Can you tell me about the organizational structure of TriageLogic?”\n",
    "# \t3.\t“Who is the CEO of TriageLogic, and what are their responsibilities?”\n",
    "\n",
    "# Policies and Procedures\n",
    "# \t4.\t“How are new policies and procedures established at TriageLogic?”\n",
    "# \t5.\t“What is the process for revising an existing policy?”\n",
    "# \t6.\t“How often are company policies reviewed and updated?”\n",
    "\n",
    "# Compliance and Regulations\n",
    "# \t7.\t“What is TriageLogic’s approach to HIPAA compliance?”\n",
    "# \t8.\t“How should employees report a potential compliance issue?”\n",
    "# \t9.\t“What is the role of the Compliance Officer at TriageLogic?”\n",
    "\n",
    "# Employee Conduct and Responsibilities\n",
    "# \t10.\t“What is the document management policy for employees?”\n",
    "# \t11.\t“Can you explain the company’s email monitoring policy?”\n",
    "# \t12.\t“What are the disciplinary actions for non-compliance with company policies?”\n",
    "\n",
    "# Client and Business Relations\n",
    "# \t13.\t“What does TriageLogic require in a client contract?”\n",
    "# \t14.\t“How does TriageLogic handle client satisfaction surveys?”\n",
    "# \t15.\t“What is the process for managing business relationships with clients?”\n",
    "\n",
    "# Training and Employment\n",
    "# \t16.\t“What are the requirements for staff orientation and training?”\n",
    "# \t17.\t“How are employee performance appraisals conducted?”\n",
    "# \t18.\t“What is the process for clinical staff credentialing?”\n",
    "\n",
    "# Data Security and IT Policies\n",
    "# \t19.\t“What is TriageLogic’s disaster recovery policy?”\n",
    "# \t20.\t“How does the company ensure data integrity and information security?”\n",
    "# \t21.\t“What are the company’s guidelines for document retention?”\n",
    "\n",
    "# Would you like me to tailor these prompts for specific employee roles, such as HR, IT, or clinical staff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: [Document(id='f6f83640-dbbe-45de-b07b-4bb5d37d6c73', metadata={'creationdate': \"D:20180605201708Z00'00'\", 'creator': 'Word', 'moddate': \"D:20180605201708Z00'00'\", 'page': 2.0, 'page_label': '3', 'producer': 'Mac OS X 10.11.6 Quartz PDFContext', 'source': 'Data/Company-Policy-and-Procedure-June-1.18-V6.0.pdf', 'title': 'Microsoft Word - Company Policy and Procedure June 1.18 V6.0.docx', 'total_pages': 109.0}, page_content='by Committee ............................................. 51 b. Quality Improvement Projects ......................................................... 54'), Document(id='d58e8cb4-709c-4e32-8680-93ddb73c91f0', metadata={'creationdate': \"D:20180605201708Z00'00'\", 'creator': 'Word', 'moddate': \"D:20180605201708Z00'00'\", 'page': 2.0, 'page_label': '3', 'producer': 'Mac OS X 10.11.6 Quartz PDFContext', 'source': 'Data/Company-Policy-and-Procedure-June-1.18-V6.0.pdf', 'title': 'Microsoft Word - Company Policy and Procedure June 1.18 V6.0.docx', 'total_pages': 109.0}, page_content='by Committee ............................................. 51 b. Quality Improvement Projects ......................................................... 54'), Document(id='2e41dd7e-135e-414a-a916-48f252c655e0', metadata={'creationdate': \"D:20180605201708Z00'00'\", 'creator': 'Word', 'moddate': \"D:20180605201708Z00'00'\", 'page': 108.0, 'page_label': '109', 'producer': 'Mac OS X 10.11.6 Quartz PDFContext', 'source': 'Data/Company-Policy-and-Procedure-June-1.18-V6.0.pdf', 'title': 'Microsoft Word - Company Policy and Procedure June 1.18 V6.0.docx', 'total_pages': 109.0}, page_content='106')]\n"
     ]
    }
   ],
   "source": [
    "# Check what the retriever is returning\n",
    "retrieved_docs = retriever.get_relevant_documents(\"Test question\")\n",
    "print(\"Retrieved Documents:\", retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio-api-key\",\n",
    "    model_name=\"deepseek-r1-distill-qwen-7b\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question, \"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Keep the answer concise, using a maximum of two sentences.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Combine the retrieved documents into a structured context\n",
    "def format_documents(docs):\n",
    "    formatted_context = \"\\n\\n\".join(\n",
    "        [f\"{doc.page_content} (Page: {int(doc.metadata.get('page', 0))})\" for doc in docs]\n",
    "    )\n",
    "    page_numbers = \", \".join(str(int(doc.metadata.get('page', 0))) for doc in docs)\n",
    "    return {\"context\": formatted_context, \"page_number\": page_numbers}\n",
    "\n",
    "# Create the chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Define the retrieval chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Query the model\n",
    "query = \"How should employees report a potential compliance issue?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)  # Fetch retrieved documents\n",
    "formatted_input = format_documents(retrieved_docs)  # Format documents\n",
    "\n",
    "# Add input query\n",
    "formatted_input[\"input\"] = query\n",
    "\n",
    "# Invoke the chain\n",
    "response = rag_chain.invoke(formatted_input)\n",
    "\n",
    "# Manually append the page numbers to the response\n",
    "page_numbers = formatted_input[\"page_number\"]\n",
    "final_answer = f\"{response['answer']} (Source: Page {page_numbers})\"\n",
    "\n",
    "# Print the response with page numbers\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compliance officer's name is **Charu Raheja**. (Source: Page 11, 21, 23, 24, 25, 94)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"http://localhost:1234/v1\",\n",
    "    api_key=\"lm-studio-api-key\",\n",
    "    model_name=\"deepseek-r1-distill-qwen-7b\",\n",
    "    temperature=0.4,\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer the question, \"\n",
    "    \"If you don't know the answer, say that you don't know. \"\n",
    "    \"Keep the answer concise, using a maximum of two sentences.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Chat Prompt Template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Combine the retrieved documents into a structured context\n",
    "def format_documents(docs):\n",
    "    # Extract and format the page numbers (increment by 1 and remove duplicates)\n",
    "    page_numbers = sorted(set(int(doc.metadata.get(\"page\", 0)) + 1 for doc in docs))\n",
    "    formatted_context = \"\\n\\n\".join(\n",
    "        [f\"{doc.page_content} (Page: {int(doc.metadata.get('page', 0)) + 1})\" for doc in docs]\n",
    "    )\n",
    "    return {\"context\": formatted_context, \"page_number\": \", \".join(map(str, page_numbers))}\n",
    "\n",
    "# Function to clean up the response (remove \"think\" sections)\n",
    "def clean_response(response_text):\n",
    "    # Remove text inside <think>...</think> if present\n",
    "    cleaned_text = re.sub(r\"<think>.*?</think>\", \"\", response_text, flags=re.DOTALL).strip()\n",
    "    return cleaned_text\n",
    "\n",
    "# Create the chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "# Define the retrieval chain\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "# Query the model\n",
    "query = \"what is the compliance officer name?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)  # Fetch retrieved documents\n",
    "formatted_input = format_documents(retrieved_docs)  # Format documents\n",
    "\n",
    "# Add input query\n",
    "formatted_input[\"input\"] = query\n",
    "\n",
    "# Invoke the chain\n",
    "response = rag_chain.invoke(formatted_input)\n",
    "\n",
    "# Clean the response and append the page numbers\n",
    "final_answer = f\"{clean_response(response['answer'])} (Source: Page {formatted_input['page_number']})\"\n",
    "\n",
    "# Print the cleaned response\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medibot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
